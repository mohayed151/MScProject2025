#Python code for creating the LTSM Model

import pandas as pd
import yfinance as yf
import numpy as np
import os
import logging
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Check pandas version
required_pandas_version = "1.0.0"
if pd.__version__ < required_pandas_version:
    logger.error(f"pandas version {pd.__version__} is outdated. Please upgrade to {required_pandas_version} or later.")
    raise ImportError(f"pandas version {pd.__version__} is outdated. Please upgrade to {required_pandas_version} or later.")

def download_sp500_intraday_data(ticker='^GSPC', period='7d', interval='1m'):
    """
    Download intraday S&P 500 data using yfinance.
    
    Args:
        ticker (str): Ticker symbol for S&P 500 (^GSPC).
        period (str): Period for data retrieval (e.g., '7d' for 7 days).
        interval (str): Data interval (e.g., '1m' for 1 minute).
    
    Returns:
        pd.DataFrame: Cleaned intraday data.
    """
    try:
        logger.info(f"Downloading intraday data for {ticker} (period={period}, interval={interval})")
        df = yf.download(ticker, period=period, interval=interval, auto_adjust=False)
        
        if df.empty:
            logger.error("No data downloaded. Check ticker, period, or interval.")
            raise ValueError("No data downloaded.")
        
        logger.info(f"Downloaded DataFrame columns: {list(df.columns)}")
        
        if isinstance(df.columns, pd.MultiIndex):
            logger.info("Multi-level columns detected. Flattening to single level.")
            df.columns = [col[0] for col in df.columns]
        
        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']
        if not all(col in df.columns for col in required_columns):
            missing_cols = [col for col in required_columns if col not in df.columns]
            logger.error(f"Missing columns in downloaded data: {missing_cols}")
            raise ValueError(f"Missing columns in downloaded data: {missing_cols}")
        
        df = df[required_columns].copy()
        df.dropna(inplace=True)
        
        df['Close'] = pd.to_numeric(df['Close'], errors='coerce')
        if df['Close'].isna().any():
            logger.warning("Non-numeric values found in Close column after conversion. Dropping invalid rows.")
            df.dropna(subset=['Close'], inplace=True)
        
        df.index = df.index.tz_convert('UTC')
        
        if df.empty:
            logger.error("DataFrame is empty after cleaning.")
            raise ValueError("DataFrame is empty after cleaning.")
        
        return df
    
    except Exception as e:
        logger.error(f"Error downloading intraday data: {str(e)}")
        raise

def preprocess_data(df, sequence_length=60):
    """
    Preprocess data for LSTM: scale features and create sequences.
    
    Args:
        df (pd.DataFrame): DataFrame with columns ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'].
        sequence_length (int): Number of time steps in each sequence.
    
    Returns:
        tuple: (scaled sequences, target values, scaler for Close)
    """
    try:
        logger.info("Preprocessing data for LSTM")
        df = df.copy()
        
        # Features to use
        feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']
        if not all(col in df.columns for col in feature_columns):
            missing_cols = [col for col in feature_columns if col not in df.columns]
            logger.error(f"Missing feature columns: {missing_cols}")
            raise ValueError(f"Missing feature columns: {missing_cols}")
        
        # Scale features
        scaler = MinMaxScaler()
        scaled_data = scaler.fit_transform(df[feature_columns])
        
        # Create scaler for Close price (for inverse transformation)
        close_scaler = MinMaxScaler()
        close_scaler.fit(df[['Close']])
        
        # Create sequences
        X, y = [], []
        for i in range(sequence_length, len(scaled_data)):
            X.append(scaled_data[i - sequence_length:i])
            y.append(scaled_data[i, feature_columns.index('Close')])  # Predict next Close price
        
        X = np.array(X)
        y = np.array(y)
        
        logger.info(f"Created {len(X)} sequences with shape {X.shape} and {len(y)} target values")
        
        return X, y, close_scaler
    
    except Exception as e:
        logger.error(f"Error preprocessing data: {str(e)}")
        raise

def build_lstm_model(sequence_length, n_features):
    """
    Build and compile an LSTM model.
    
    Args:
        sequence_length (int): Number of time steps in each sequence.
        n_features (int): Number of features in each time step.
    
    Returns:
        keras.Model: Compiled LSTM model.
    """
    try:
        logger.info("Building LSTM model")
        model = Sequential([
            LSTM(50, return_sequences=True, input_shape=(sequence_length, n_features)),
            Dropout(0.2),
            LSTM(50),
            Dropout(0.2),
            Dense(25),
            Dense(1)  # Output: next Close price
        ])
        
        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
        model.summary(print_fn=lambda x: logger.info(x))
        
        return model
    
    except Exception as e:
        logger.error(f"Error building LSTM model: {str(e)}")
        raise

def train_lstm_model(X, y, epochs=50, batch_size=32):
    """
    Train the LSTM model.
    
    Args:
        X (np.array): Input sequences.
        y (np.array): Target values (scaled Close prices).
        epochs (int): Number of training epochs.
        batch_size (int): Batch size for training.
    
    Returns:
        keras.Model: Trained LSTM model.
    """
    try:
        logger.info(f"Training LSTM model for {epochs} epochs with batch size {batch_size}")
        
        # Split data (80% train, 20% validation)
        train_size = int(0.8 * len(X))
        X_train, X_val = X[:train_size], X[train_size:]
        y_train, y_val = y[:train_size], y[train_size:]
        
        model = build_lstm_model(sequence_length=X.shape[1], n_features=X.shape[2])
        
        # Train model
        history = model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_val, y_val),
            verbose=1
        )
        
        # Log training performance
        final_train_loss = history.history['loss'][-1]
        final_val_loss = history.history['val_loss'][-1]
        logger.info(f"Final training loss: {final_train_loss:.6f}")
        logger.info(f"Final validation loss: {final_val_loss:.6f}")
        
        return model
    
    except Exception as e:
        logger.error(f"Error training LSTM model: {str(e)}")
        raise

def main():
    """Main function to download intraday data, train LSTM model, and save it."""
    try:
        # Download intraday data
        df = download_sp500_intraday_data()
        logger.info(f"Intraday data shape: {df.shape}")
        
        # Preprocess data
        sequence_length = 60
        X, y, close_scaler = preprocess_data(df, sequence_length=sequence_length)
        
        # Train LSTM model
        model = train_lstm_model(X, y, epochs=50, batch_size=32)
        
        # Save model
        os.makedirs('models', exist_ok=True)
        model_path = 'models/ltsm.h5'
        model.save(model_path)
        logger.info(f"LSTM model saved to {model_path}")
        
        # Save scaler for future use
        scaler_path = 'models/close_scaler.joblib'
        joblib.dump(close_scaler, scaler_path)
        logger.info(f"Close scaler saved to {scaler_path}")
        
        return df, model, close_scaler
    
    except Exception as e:
        logger.error(f"Error in main function: {str(e)}")
        raise

if __name__ == "__main__":
    df, model, close_scaler = main()
    print("Intraday data head:\n", df.head())
