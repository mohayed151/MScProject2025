#Code to develop the XGBoost Model

import pandas as pd
import yfinance as yf
import numpy as np
import os
import logging
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
import joblib

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Check pandas version
required_pandas_version = "1.0.0"
if pd.__version__ < required_pandas_version:
    logger.error(f"pandas version {pd.__version__} is outdated. Please upgrade to {required_pandas_version} or later.")
    raise ImportError(f"pandas version {pd.__version__} is outdated. Please upgrade to {required_pandas_version} or later.")

def download_sp500_intraday_data(ticker='^GSPC', period='7d', interval='1m'):
    """
    Download intraday S&P 500 data using yfinance.
    
    Args:
        ticker (str): Ticker symbol for S&P 500 (^GSPC).
        period (str): Period for data retrieval (e.g., '7d' for 7 days).
        interval (str): Data interval (e.g., '1m' for 1 minute).
    
    Returns:
        pd.DataFrame: Cleaned intraday data.
    """
    try:
        logger.info(f"Downloading intraday data for {ticker} (period={period}, interval={interval})")
        df = yf.download(ticker, period=period, interval=interval, auto_adjust=False)
        
        if df.empty:
            logger.error("No data downloaded. Check ticker, period, or interval.")
            raise ValueError("No data downloaded.")
        
        logger.info(f"Downloaded DataFrame columns: {list(df.columns)}")
        
        if isinstance(df.columns, pd.MultiIndex):
            logger.info("Multi-level columns detected. Flattening to single level.")
            df.columns = [col[0] for col in df.columns]
        
        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']
        if not all(col in df.columns for col in required_columns):
            missing_cols = [col for col in required_columns if col not in df.columns]
            logger.error(f"Missing columns in downloaded data: {missing_cols}")
            raise ValueError(f"Missing columns in downloaded data: {missing_cols}")
        
        df = df[required_columns].copy()
        df.dropna(inplace=True)
        
        df['Close'] = pd.to_numeric(df['Close'], errors='coerce')
        if df['Close'].isna().any():
            logger.warning("Non-numeric values found in Close column after conversion. Dropping invalid rows.")
            df.dropna(subset=['Close'], inplace=True)
        
        df.index = df.index.tz_convert('UTC')
        
        if df.empty:
            logger.error("DataFrame is empty after cleaning.")
            raise ValueError("DataFrame is empty after cleaning.")
        
        return df
    
    except Exception as e:
        logger.error(f"Error downloading intraday data: {str(e)}")
        raise

def preprocess_data(df, sequence_length=60):
    """
    Preprocess data for XGBoost: scale features and create flattened sequences.
    
    Args:
        df (pd.DataFrame): DataFrame with columns ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'].
        sequence_length (int): Number of time steps in each sequence.
    
    Returns:
        tuple: (flattened sequences, target values, scaler for Close)
    """
    try:
        logger.info("Preprocessing data for XGBoost")
        df = df.copy()
        
        # Features to use
        feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']
        if not all(col in df.columns for col in feature_columns):
            missing_cols = [col for col in feature_columns if col not in df.columns]
            logger.error(f"Missing feature columns: {missing_cols}")
            raise ValueError(f"Missing feature columns: {missing_cols}")
        
        # Scale features
        scaler = MinMaxScaler()
        scaled_data = scaler.fit_transform(df[feature_columns])
        
        # Create scaler for Close price
        close_scaler = MinMaxScaler()
        close_scaler.fit(df[['Close']])
        
        # Create sequences and flatten them
        X, y = [], []
        for i in range(sequence_length, len(scaled_data)):
            X.append(scaled_data[i - sequence_length:i].flatten())  # Flatten [sequence_length, n_features] to 1D
            y.append(scaled_data[i, feature_columns.index('Close')])  # Next Close price
        
        X = np.array(X)
        y = np.array(y)
        
        logger.info(f"Created {len(X)} sequences with shape {X.shape} and {len(y)} target values")
        
        return X, y, close_scaler
    
    except Exception as e:
        logger.error(f"Error preprocessing data: {str(e)}")
        raise

def train_xgboost_model(X, y):
    """
    Train an XGBoost regressor model.
    
    Args:
        X (np.array): Flattened input sequences.
        y (np.array): Target values (scaled Close prices).
    
    Returns:
        XGBRegressor: Trained model.
    """
    try:
        logger.info("Training XGBoost model")
        
        # Split data (80% train, 20% validation)
        train_size = int(0.8 * len(X))
        X_train, X_val = X[:train_size], X[train_size:]
        y_train, y_val = y[:train_size], y[train_size:]
        
        # Initialize and train model
        model = XGBRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42
        )
        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse', verbose=False)
        
        # Evaluate model
        y_pred_train = model.predict(X_train)
        y_pred_val = model.predict(X_val)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
        val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))
        
        logger.info(f"Training RMSE: {train_rmse:.6f}")
        logger.info(f"Validation RMSE: {val_rmse:.6f}")
        
        return model
    
    except Exception as e:
        logger.error(f"Error training XGBoost model: {str(e)}")
        raise

def main():
    """Main function to download intraday data, train XGBoost model, and save it."""
    try:
        # Download intraday data
        df = download_sp500_intraday_data()
        logger.info(f"Intraday data shape: {df.shape}")
        
        # Preprocess data
        sequence_length = 60
        X, y, close_scaler = preprocess_data(df, sequence_length=sequence_length)
        
        # Train XGBoost model
        model = train_xgboost_model(X, y)
        
        # Save model
        os.makedirs('models', exist_ok=True)
        model_path = 'models/xgboost_model.json'
        model.save_model(model_path)
        logger.info(f"XGBoost model saved to {model_path}")
        
        # Save scaler for future use
        scaler_path = 'models/close_scaler.joblib'
        joblib.dump(close_scaler, scaler_path)
        logger.info(f"Close scaler saved to {scaler_path}")
        
        return df, model, close_scaler
    
    except Exception as e:
        logger.error(f"Error in main function: {str(e)}")
        raise

if __name__ == "__main__":
    df, model, close_scaler = main()
    print("Intraday data head:\n", df.head())
